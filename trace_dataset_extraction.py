# -*- coding: utf-8 -*-
"""trace_dataset_extraction.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1X0rRm_wiHMIaMdAfGh1xZZXACsHM8XIb

# **Brief Introduction**

> In this section we explain the metrics that are being extracted as long as the approach used to achieve the end result.

### Feature metrics
```
Counts:
- # clusters
- # invocations: number of calls done to any clusters
- # lock invocations: cluster calls that have a semantic lock
- # read invocations: 
- # operations: number of times that an entity is accessed
- # read operations: number of times that an entity is accessed to be read
- # write operations: number of times that an entity is accessed to be written

Averages:
- Average # of invocation operations: average number of entity operations that a cluster call does
- Average # of invocation read operations
- Average # of invocation write operations
- Average # of pivot invocations per cluster
```

### Cluster metrics
```
Counts:
- [CLI] - # lock invocations: Total number of invocations that have one or more write operations
- [CRI] - # read invocations: Total number of invocations with only read operations, and no semantic lock
- [CI] - # invocations: Total number of times that a cluster appears in the trace
- [CRO] - # read operations: Total number of read operations in any given entity
- [CWO] - # write operations: Total number of write operations in any given entity
- [CO] - # operations: Total number of opperations in the entities of the cluster
- [CPI] - # pivot_invocations: Total number of external clusters accessed between all cluster invocations

Averages:
- [ACIO] - Average # of invocation operations: Average number of total entity operations per cluster invocations
- [ACIRO] - Average # invocation read operations: Average number of read entity operations per cluster invocations
- [ACIWO] - Average # invocation write operations: Average number of write entity operations per cluster invocations
- [ACPI] - Average # pivot invocations: Average number of other clusters called between each cluster invocation

Inner-cluster probabilities:
- [CLIP] - Lock invocation probability: probability of an invocation of this cluster having a semantic lock
- [CRIP] - Read invocation probability: probability of an invocation of this cluster not having semantic lock
- [CROP] - Read operation probability: probability of an entity operation in this cluster being a read
- [CWOP] - Write operation probability: probability of an entity operation in this cluster being a write

Feature/cluster probabilities:
- [CIP] - Invocation probability: probability of this cluster being invoqued
- [COP] - Operation probability: probability of an entity operation being done in this cluster

Factors:
- [CPIF] - Pivot invocations factor: factor between the average pivot invocations on this cluster and the average pivot invocations of the entire feature
- [CIOF] - Invocation operations factor: factor between the average invocation operations of this cluster and the average invocation operations of the entire feature
```

# **Script configuration**
"""

import os
import json
import matplotlib.pyplot as plt
import numpy as np
import tensorflow as tf

entity_operation_id = 0

TRACES_FOLDER = "drive/MyDrive/Tese/data/traces"
CODEBASES = ["LdoD", "BW"]

# feature names should not include file extension eg: ["removeTweets"]
# to extract all features in a codebase insert []
FEATURES_TO_EXTRACT = [
                [],
                []
]

# select the feature that will be analysed in the 
# data visualization part of the script
FEATURE_TO_VISUALIZE = "getTaxonomy"

# choose if a .csv file with the extracted metrics should be generated
GENERATE_CSV = True

class CSV_FORMATS():
  METRICS = "metrics"
  OPERATIONS = "operations"

CSV_FORMAT = CSV_FORMATS.OPERATIONS

"""# **Data representations**

We will now define the main structures that will be used to store data on the feature and clusters
"""

class EntityMetrics():

    def __init__(self):
        # counts
        self.read_operations = 0                          # Total number of read operations
        self.write_operations = 0                         # Total number of write operations
        self.operations = 0                               # Total number of times that an entity is accessed
        self.pivot_operations = 0                  # Total number of other entities called between this entity operations
        # averages
        self.average_operations = 0.0    # Average number of times that an entity is accessed per cluster invocation
        self.average_pivot_operations = 0.0              # Average number of other entities accessed between entity operations


class Entity():
    
    def __init__(self, cluster_name, name):
        self.cluster_name = cluster_name
        self.name = name
        self.metrics = EntityMetrics()
        self.operation_ids = list()
        self.invocation_ids = list()

class Cluster():

    def __init__(self, name):
        self.name = name
        self.metrics = ClusterMetrics()
        self.entities = dict()
        self.invocation_ids = list()
        self.is_orchestrator = False

    def get_or_create_entity(self, entity_name: str) -> Entity:
        if not self.entities.get(entity_name):
            self.entities[entity_name] = Entity(cluster_name=self.name, name=entity_name)
        
        return self.entities[entity_name]
    
    def set_as_orchestrator(self):
      self.is_orchestrator = True

class ClusterMetrics():
    
    def __init__(self):
        # counts
        self.lock_invocations = 0                       # Total number of invocations that have one or more write operations
        self.read_invocations = 0                       # Total number of invocations with only read operations, and no semantic lock
        self.invocations = 0                            # Total number of times that a cluster appears in the trace
        self.read_operations = 0                        # Total number of read operations in any given entity
        self.write_operations = 0                       # Total number of write operations in any given entity
        self.operations = 0                             # Total number of opperations in the entities of the cluster
        self.pivot_invocations = 0                      # Total number of external clusters accessed between all cluster invocations
        # averages
        self.average_invocation_operations = 0.0        # Average number of total entity operations per cluster invocations
        self.average_invocation_read_operations = 0.0   # Average number of read entity operations per cluster invocations
        self.average_invocation_write_operations = 0.0  # Average number of write entity operations per cluster invocations
        self.average_pivot_invocations = 0.0            # Average number of other clusters called between each cluster invocation
        # cluster probabilities
        self.lock_invocation_probability = 0.0
        self.read_invocation_probability = 0.0
        self.read_operation_probability = 0.0
        self.write_operation_probability = 0.0
        # feature-cluster probabilities
        self.invocation_probability = 0.0
        self.operation_probability = 0.0
        # factors
        self.pivot_invocations_factor = 0.0
        self.invocation_operations_factor = 0.0
  
class Invocation():

    def __init__(self, cluster: Cluster, entity: Entity, operation):
      self.cluster = cluster
      self.entity = entity
      self.operation = operation

class Feature():

    def __init__(self, codebase, name):
        self.codebase = codebase
        self.name = name
        self.metrics = FeatureMetrics()
        self.clusters = dict()
        self.orchestrator = None

        self.entity_invocations = []

    def get_or_create_cluster(self, cluster_name: str) -> Cluster:
        if not self.clusters.get(cluster_name):
            self.clusters[cluster_name] = Cluster(name=cluster_name)
        
        return self.clusters[cluster_name]
    
    def set_orchestrator(self, cluster_name: str):
      self.orchestrator = cluster_name
      for _, cluster in self.clusters.items():
        if cluster.name == cluster_name:
          cluster.set_as_orchestrator()
    
    def add_entity_invocation(self, invocation: Invocation):
      self.entity_invocations.append(invocation)

class FeatureMetrics():

    def __init__(self):
        self.complexity = 0.0
        # counts
        self.clusters = 0
        self.lock_invocations = 0
        self.read_invocations = 0
        self.invocations = 0
        self.read_operations = 0
        self.write_operations = 0
        self.operations = 0
        # averages
        self.average_invocation_operations = 0.0
        self.average_invocation_read_operations = 0.0
        self.average_invocation_write_operations = 0.0
        self.average_pivot_invocations = 0.0

"""# âž— **Metrics calculation**"""

def calculate_invocation_metrics(feature: Feature, cluster: Cluster, trace_invocation): 
    global entity_operation_id
    has_semantick_lock = False

    feature.metrics.invocations += 1
    cluster.metrics.invocations += 1

    entity_operations = json.loads(trace_invocation["accessedEntities"])
    for entity_operation in entity_operations:
        entity_name = entity_operation[0]
        operation = entity_operation[1]

        entity = cluster.get_or_create_entity(entity_name)

        feature.add_entity_invocation(
            invocation= Invocation(cluster, entity, operation)
        )

        entity.invocation_ids.append(cluster.invocation_ids[len(cluster.invocation_ids) - 1])
        entity.operation_ids.append(entity_operation_id)
        entity_operation_id += 1

        feature.metrics.operations += 1
        cluster.metrics.operations += 1
        entity.metrics.operations += 1

        if operation == "R":
            feature.metrics.read_operations += 1
            cluster.metrics.read_operations += 1
            entity.metrics.read_operations += 1

        elif operation == "W":
            feature.metrics.write_operations += 1
            cluster.metrics.write_operations += 1
            entity.metrics.write_operations += 1
            has_semantick_lock = True

    if has_semantick_lock:
        feature.metrics.lock_invocations += 1
        cluster.metrics.lock_invocations += 1
    else:
        feature.metrics.read_invocations += 1
        cluster.metrics.read_invocations += 1

def calculate_cluster_averages(feature: Feature, cluster: Cluster):
    cluster.metrics.average_invocation_operations = float(cluster.metrics.operations / cluster.metrics.invocations)
    cluster.metrics.average_invocation_read_operations = float(cluster.metrics.read_operations / cluster.metrics.invocations)
    cluster.metrics.average_invocation_write_operations = float(cluster.metrics.write_operations / cluster.metrics.invocations)

    if cluster.metrics.invocations > 1:
        cluster.metrics.average_pivot_invocations = float(cluster.metrics.pivot_invocations / (cluster.metrics.invocations - 1))


def calculate_cluster_probabilities(feature: Feature, cluster: Cluster):
    cluster.metrics.lock_invocation_probability = float(cluster.metrics.lock_invocations / cluster.metrics.invocations)
    cluster.metrics.read_invocation_probability = float(cluster.metrics.read_invocations / cluster.metrics.invocations)
    cluster.metrics.read_operation_probability = float(cluster.metrics.read_operations / cluster.metrics.operations)
    cluster.metrics.write_operation_probability = float(cluster.metrics.write_operations / cluster.metrics.operations)
    cluster.metrics.invocation_probability = float(cluster.metrics.invocations / feature.metrics.invocations)
    cluster.metrics.operation_probability = float(cluster.metrics.operations / feature.metrics.operations)


def calculate_cluster_factors(feature: Feature, cluster: Cluster):
    cluster.metrics.pivot_invocations_factor = float(cluster.metrics.average_pivot_invocations / feature.metrics.average_pivot_invocations)
    cluster.metrics.invocation_operations_factor = float(cluster.metrics.average_invocation_operations / feature.metrics.average_invocation_operations)

def update_feature_averages(feature: Feature, cluster: Cluster):
    feature.metrics.average_invocation_operations += float(cluster.metrics.average_invocation_operations / feature.metrics.clusters)
    feature.metrics.average_invocation_read_operations += float(cluster.metrics.average_invocation_read_operations / feature.metrics.clusters)
    feature.metrics.average_invocation_write_operations += float(cluster.metrics.average_invocation_write_operations / feature.metrics.clusters)
    feature.metrics.average_pivot_invocations += float(cluster.metrics.average_pivot_invocations / feature.metrics.clusters)

def calculate_entity_averages(cluster: Cluster, entity: Entity):
    entity.metrics.average_operations = float(entity.metrics.operations / cluster.metrics.invocations)

    # entity.metrics.average_pivot_entity_operations

def calculate_final_metrics(feature: Feature):
    for _, cluster in feature.clusters.items():
        cluster.metrics.pivot_invocations = feature.metrics.invocations - cluster.metrics.invocations - cluster.invocation_ids[0] - (feature.metrics.invocations - cluster.invocation_ids[len(cluster.invocation_ids) - 1] - 1) # WE HAVE TO REMOVE THE NUMBER OF INVOCATIONS BEFORE THE FIRST ONE AND AFTER THE LAST ONE OF THIS CLUSTER

        calculate_cluster_averages(feature, cluster)
        calculate_cluster_probabilities(feature, cluster)

        update_feature_averages(feature, cluster)
        
        for _, entity in cluster.entities.items():
            calculate_entity_averages(cluster, entity)

    for _, cluster in feature.clusters.items():
        calculate_cluster_factors(feature, cluster)

"""# âœ… **Main script execution**

Next we define a method to **extract the data from each one of the trace files** into a list. 
This will contain all of the traces that will then be analyzed
"""

def read_file(path):
  try:
    with open(path) as json_file:
        trace = json.load(json_file)
        return trace
  except Exception as e:
    print(f"Failed to extract feature trace from path: {path} | Error: {e}")
    return None
    
def read_trace_files():
  data = list()
  for idx, codebase in enumerate(CODEBASES):
    # extract all features in codebase if []
    if len(FEATURES_TO_EXTRACT[idx]) == 0:
      for file in os.listdir(os.fsencode(f"{TRACES_FOLDER}/{codebase}")):
        filename = os.fsdecode(file)
        if filename.endswith(".json"): 
          trace = read_file(f"{TRACES_FOLDER}/{codebase}/{filename}")
          if trace:
            trace['codebase'] = codebase
            data.append(trace)
  
    else:
      for feature_name in FEATURES_TO_EXTRACT[idx]:
        trace = read_file(f"{TRACES_FOLDER}/{codebase}/{feature_name}.json")
        if trace:
          trace['codebase'] = codebase
          data.append(trace)
  
  return data

def extract_trace_metrics(trace_data) -> Feature:
  feature = Feature(codebase=trace_data["codebase"], name=trace_data["name"])
  feature.metrics.complexity = float(trace_data["complexity"])

  for trace_invocation in trace_data["functionalityRedesigns"][0]["redesign"]:
      invocation_id = int(trace_invocation["id"])
      if invocation_id == -1:
          continue

      if not feature.clusters.get(trace_invocation["cluster"]):
          feature.metrics.clusters += 1

      cluster = feature.get_or_create_cluster(trace_invocation["cluster"])
      cluster.invocation_ids.append(invocation_id)

      calculate_invocation_metrics(feature, cluster, trace_invocation)
  
  # if contains redesign, we find the orchestrator
  lowest_id = None
  orchestrator = None
  if len(trace_data["functionalityRedesigns"]) > 1:
    for trace_invocation in trace_data["functionalityRedesigns"][1]["redesign"]:
      if len(trace_invocation["remoteInvocations"]) > 1:
        if not lowest_id or trace_invocation["id"] < lowest_id:
          orchestrator = trace_invocation["cluster"]
          lowest_id = trace_invocation["id"]

    feature.set_orchestrator(orchestrator)
  

  calculate_final_metrics(feature)
  return feature

from operator import itemgetter

trace_data = read_trace_files()

# Print list of highest complexity traces:
traces_sorted_by_complexity = sorted(trace_data, key=itemgetter('complexity'), reverse = True)
print("\n5 highest complexity traces:")
for trace in traces_sorted_by_complexity[:5]:
  print(f'- {trace["name"]} - {trace["complexity"]}')

# Calculate metrics
features = list()
for trace in trace_data:
  feature = extract_trace_metrics(trace)
  features.append(feature)

"""# ðŸ“Š **Data visualization**

First we set the feature we want to visualize
"""

found_feature = False
for feature in features:
  if FEATURE_TO_VISUALIZE in feature.name:
    found_feature = True
    break

if not found_feature:
  print(f"Feature to visualize not found!")
else:
  print(f"Found feature to visualize: {feature.name}")

"""## Plot helper methods"""

def plot_table(cellText, colLabels):
  fig, ax = plt.subplots()
  fig.set_figheight(1)

  # hide axes
  fig.patch.set_visible(False)
  ax.axis('off')
  ax.axis('tight')
  the_table = ax.table(cellText=cellText, colLabels=colLabels, loc='center')
  the_table.scale(2, 2)


def plot_multiple_bars(
    y_values_matrix, y_values_labels, y_legend, x_values, x_legend,
):
  # set width of bar 
  barWidth = 0.15
  fig = plt.subplots(figsize =(8, 4)) 

  colors = ['bisque', 'rosybrown', 'palegreen', 'turquoise', 'plum']
    
  # Set position of bar on X axis 
  br = [np.arange(len(y_values_matrix[0]))]
  for idx, _ in enumerate(y_values_matrix):
    br.append([x + barWidth for x in br[idx]]) 
    
  # Make the plot
  for idx, values in enumerate(y_values_matrix):
    plt.bar(br[idx], values, color = colors[idx], width = barWidth, 
            edgecolor = 'grey', label = y_values_labels[idx]) 

  # Adding Xticks  
  plt.xlabel(x_legend, fontweight ='bold') 
  plt.ylabel( y_legend, fontweight ='bold') 
  plt.xticks([r + barWidth for r in range(len(y_values_matrix[0]))], x_values) 
  leg = plt.legend()

  plt.show()


def plot_probability_stacked_bars(
    y_values_matrix, y_values_labels, y_legend, x_values, x_legend
):
  N = len(y_values_matrix[0])

  ind = np.arange(N)    
  width = 0.35  
    
  fig = plt.subplots(figsize = (8, 5)) 
  
  t = ()
  for idx, values in enumerate(y_values_matrix):
    if idx == 0:
      p = plt.bar(ind, values, width) 
    else:
      p = plt.bar(ind, values, width, bottom = y_values_matrix[idx-1]) 
    t += (p[0],)

  plt.ylabel(y_legend) 
  plt.title(x_legend) 
  plt.xticks(ind, x_values) 
  plt.yticks(np.arange(0, 1.3, 0.1)) 
  plt.legend(t, y_values_labels) 
    
  plt.show()

"""## Feature metrics"""

print("Feature name:\n",feature.name)
print("\nFeature metrics:")

plot_table(
    cellText=[
              [
                feature.metrics.complexity,
                feature.metrics.clusters,
                feature.metrics.invocations,
                feature.metrics.operations,
               feature.orchestrator,
              ]
            ],
    colLabels=[
                "Complexity",
                "Clusters",
                "Invocations",
                "Operations",
                "Redesign orchestrator",
              ],
)

"""## Cluster metrics

### Counts
"""

print("Feature name:\n",feature.name)
print("\nCluster counts:")

plot_table(
    cellText=[
              [
                cluster.name,
                cluster.metrics.invocations,
                cluster.metrics.lock_invocations,
                cluster.metrics.read_invocations,
                cluster.metrics.operations,
                cluster.metrics.read_operations,
                cluster.metrics.write_operations, 
                cluster.metrics.pivot_invocations,
              ] for _, cluster in feature.clusters.items()
            ],
    colLabels=[
                "Name",
                "CI",
                "CLI",
                "CRI",
                "CO",
                "CRO",
                "CWO",
                "CPI",
              ],
)

"""### Averages"""

print("Feature name:\n",feature.name)
print("\nCluster averages:")

# table
averages_matrix = [
                   [
                    cluster.name,
                    cluster.metrics.average_invocation_operations,
                    cluster.metrics.average_invocation_read_operations,
                    cluster.metrics.average_invocation_write_operations,
                    cluster.metrics.average_pivot_invocations,
                   ] for _, cluster in feature.clusters.items()
]

plot_table(
    cellText=averages_matrix,
    colLabels=["Name", "ACIO", "ACIRO", "ACIWO", "ACPI"],
)

# bar plot
averages_matrix = [
                   [cluster.metrics.average_invocation_operations for _, cluster in feature.clusters.items()],
                   [cluster.metrics.average_invocation_read_operations for _, cluster in feature.clusters.items()],
                   [cluster.metrics.average_invocation_write_operations for _, cluster in feature.clusters.items()],
                   [cluster.metrics.average_pivot_invocations for _, cluster in feature.clusters.items()],
]

plot_multiple_bars(
    y_values_matrix=averages_matrix,
    y_values_labels=["ACIO","ACIRO","ACIWO", "ACPI"],
    y_legend="Averages",
    x_values=[cluster.name for _, cluster in feature.clusters.items()],
    x_legend="Cluster name",
)

"""### Inside-cluster Probabilities"""

print("Feature name:\n",feature.name)
print("\nInner cluster probabilities:")

plot_table(
    cellText=[
              [
                cluster.name,
                cluster.metrics.lock_invocation_probability,
                cluster.metrics.read_invocation_probability,
                cluster.metrics.read_operation_probability,
                cluster.metrics.write_operation_probability,
              ] for _, cluster in feature.clusters.items()
            ],
    colLabels=["Name", "CLIP", "CRIP", "CROP", "CWOP"],
)


plot_probability_stacked_bars(
    y_values_matrix=[
                   [cluster.metrics.lock_invocation_probability for _, cluster in feature.clusters.items()],
                   [cluster.metrics.read_invocation_probability for _, cluster in feature.clusters.items()],
    ],
    y_values_labels=("CLIP", "CRIP"),
    y_legend="Probability",
    x_values=(cluster.name for _, cluster in feature.clusters.items()),
    x_legend="Probability by cluster",
)  

plot_probability_stacked_bars(
    y_values_matrix=[
                   [cluster.metrics.write_operation_probability for _, cluster in feature.clusters.items()],
                   [cluster.metrics.read_operation_probability for _, cluster in feature.clusters.items()],
    ],
    y_values_labels=("CWOP", "CROP"),
    y_legend="Probability",
    x_values=(cluster.name for _, cluster in feature.clusters.items()),
    x_legend="Probability by cluster",
)

"""### Outside-cluster probabilities"""

print("Feature name:\n",feature.name)
print("\nCluster/Feature probabilities:")

plot_table(
    cellText=[
              [
                cluster.name,
                cluster.metrics.invocation_probability,
                cluster.metrics.operation_probability,
              ] for _, cluster in feature.clusters.items()
            ],
    colLabels=[
                "Name",
                "CIP",
                "COP",
              ],
)

plot_probability_stacked_bars(
    y_values_matrix=[
                   [
                    cluster.metrics.invocation_probability, 
                    cluster.metrics.operation_probability,
                   ] for _, cluster in feature.clusters.items()
    ],
    y_values_labels=(cluster.name for _, cluster in feature.clusters.items()),
    y_legend="Probability",
    x_values=("CIP", "COP"),
    x_legend="Probability by cluster",
)

"""### Factors"""

print("Feature name:\n",feature.name)
print("\nCluster factors:")

plot_table(
    cellText=[
              [
                cluster.name,
                cluster.metrics.pivot_invocations_factor,
                cluster.metrics.invocation_operations_factor,
              ] for _, cluster in feature.clusters.items()
            ],
    colLabels=[
                "Name",
                "CPIF",
                "CIOF",
              ],
)

plot_multiple_bars(
    y_values_matrix=[
                   [cluster.metrics.pivot_invocations_factor for _, cluster in feature.clusters.items()],
    ],
    y_values_labels=["CPIF"],
    y_legend="Factors",
    x_values=[cluster.name for _, cluster in feature.clusters.items()],
    x_legend="Cluster name",
)

plot_multiple_bars(
    y_values_matrix=[
                   [cluster.metrics.invocation_operations_factor for _, cluster in feature.clusters.items()],
    ],
    y_values_labels=["CIOF"],
    y_legend="Factors",
    x_values=[cluster.name for _, cluster in feature.clusters.items()],
    x_legend="Cluster name",
)

"""# ðŸ“ **Data extraction**

The training vector we are using has the following format. Features are identified in the first column and the clusters in the second. Then we follow the data with the metrics we are using to train the model.


In this part we want to generate a **.csv file** that can then be loaded into our Tensorflow engine.


**Filename format**: `trace_analysis-18/12/2020-20:40.csv`

| Codebase | Feature | Cluster | CLIP  | CRIP | CROP  | CWOP | CIP | COP | CPIF | CIOF | Orchestrator
|---|---|---|---|---|---|---|---|---|---|---|---|
| LdoD| AdminController.removeTweets | Text | 0.4 | 0.6 | 0.33 | 0.66 | 0.2 | 0.4 | 1.4 | 0.8 | 1 |
| LdoD| AdminController.removeTweets | VirtualEdition  | 0.4 | 0.6 | 0.33 | 0.66 | 0.3 | 0.4 | 1.4 | 0.8 | 0 |			
| LdoD| AdminController.removeTweets |  Twitter | 0.4 | 0.6 | 0.33 | 0.66 | 0.4 | 0.3 | 0.7 | 0.2 |	0 |
"""

if GENERATE_CSV:
  import csv
  from datetime import datetime
  filename = f'trace_{CSV_FORMAT}-{datetime.now().strftime("%d_%m_%Y_%H_%M")}.csv'

  with open(filename, 'w', newline='') as file:
      writer = csv.writer(file)

      if CSV_FORMAT == CSV_FORMATS.METRICS:
        writer.writerow(["Codebase", "Feature", "Cluster", "CLIP", "CRIP", "CROP", "CWOP", "CIP", "COP", "CPIF", "CIOF", "Orchestrator"])
        
        for feature in features:
          for _, cluster in feature.clusters.items():
            writer.writerow(
                [
                feature.codebase,
                feature.name,
                cluster.name,
                cluster.metrics.lock_invocation_probability,
                cluster.metrics.read_invocation_probability,
                cluster.metrics.read_operation_probability,
                cluster.metrics.write_operation_probability,
                cluster.metrics.invocation_probability,
                cluster.metrics.operation_probability,
                cluster.metrics.pivot_invocations_factor,
                cluster.metrics.invocation_operations_factor,
                1 if cluster.is_orchestrator else 0,
                ]
            )

      elif CSV_FORMAT == CSV_FORMATS.OPERATIONS:
          writer.writerow(["Codebase", "Feature", "Cluster", "Entity", "Operation", "Orchestrator"])
    
          for feature in features:
            for invocation in feature.entity_invocations:
              writer.writerow(
                  [
                  feature.codebase,
                  feature.name,
                  invocation.cluster.name,
                  invocation.entity.name,
                  invocation.operation,
                  1 if invocation.cluster.is_orchestrator else 0,
                  ]
              )