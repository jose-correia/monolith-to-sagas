{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6DhUzc1JpEhS"
   },
   "source": [
    "# Script configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "id": "gusVhoCtopNm"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pip:****@pypi.infra.unbabel.com/simple/\n",
      "Requirement already satisfied: pandas in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (1.2.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from pandas) (2.8.1)\n",
      "Requirement already satisfied: numpy>=1.16.5 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from pandas) (1.19.5)\n",
      "Requirement already satisfied: pytz>=2017.3 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from pandas) (2021.1)\n",
      "Requirement already satisfied: six>=1.5 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from python-dateutil>=2.7.3->pandas) (1.15.0)\n",
      "\u001b[33mWARNING: You are using pip version 20.1; however, version 21.0.1 is available.\n",
      "You should consider upgrading via the '/usr/local/bin/python3.8 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Looking in indexes: https://pypi.org/simple, https://pip:****@pypi.infra.unbabel.com/simple/\n",
      "Requirement already satisfied: matplotlib in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (3.4.0)\n",
      "Requirement already satisfied: numpy>=1.16 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from matplotlib) (1.19.5)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from matplotlib) (8.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from matplotlib) (2.8.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from matplotlib) (0.10.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from matplotlib) (1.3.1)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from matplotlib) (2.4.7)\n",
      "Requirement already satisfied: six>=1.5 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from python-dateutil>=2.7->matplotlib) (1.15.0)\n",
      "\u001b[33mWARNING: You are using pip version 20.1; however, version 21.0.1 is available.\n",
      "You should consider upgrading via the '/usr/local/bin/python3.8 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Looking in indexes: https://pypi.org/simple, https://pip:****@pypi.infra.unbabel.com/simple/\n",
      "Requirement already satisfied: sklearn in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (0.0)\n",
      "Requirement already satisfied: scikit-learn in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from sklearn) (0.24.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from scikit-learn->sklearn) (2.1.0)\n",
      "Requirement already satisfied: scipy>=0.19.1 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from scikit-learn->sklearn) (1.6.2)\n",
      "Requirement already satisfied: joblib>=0.11 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from scikit-learn->sklearn) (1.0.1)\n",
      "Requirement already satisfied: numpy>=1.13.3 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from scikit-learn->sklearn) (1.19.5)\n",
      "\u001b[33mWARNING: You are using pip version 20.1; however, version 21.0.1 is available.\n",
      "You should consider upgrading via the '/usr/local/bin/python3.8 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Looking in indexes: https://pypi.org/simple, https://pip:****@pypi.infra.unbabel.com/simple/\n",
      "Requirement already satisfied: tensorflow in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (2.4.1)\n",
      "Requirement already satisfied: google-pasta~=0.2 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: wheel~=0.35 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from tensorflow) (0.36.2)\n",
      "Requirement already satisfied: wrapt~=1.12.1 in /Users/josecorreia/Library/Python/3.8/lib/python/site-packages (from tensorflow) (1.12.1)\n",
      "Requirement already satisfied: numpy~=1.19.2 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from tensorflow) (1.19.5)\n",
      "Requirement already satisfied: tensorflow-estimator<2.5.0,>=2.4.0 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from tensorflow) (2.4.0)\n",
      "Requirement already satisfied: typing-extensions~=3.7.4 in /Users/josecorreia/Library/Python/3.8/lib/python/site-packages (from tensorflow) (3.7.4.3)\n",
      "Requirement already satisfied: astunparse~=1.6.3 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: opt-einsum~=3.3.0 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from tensorflow) (3.3.0)\n",
      "Requirement already satisfied: protobuf>=3.9.2 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from tensorflow) (3.12.2)\n",
      "Requirement already satisfied: tensorboard~=2.4 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from tensorflow) (2.4.1)\n",
      "Requirement already satisfied: gast==0.3.3 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from tensorflow) (0.3.3)\n",
      "Requirement already satisfied: grpcio~=1.32.0 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from tensorflow) (1.32.0)\n",
      "Requirement already satisfied: six~=1.15.0 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from tensorflow) (1.15.0)\n",
      "Requirement already satisfied: h5py~=2.10.0 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from tensorflow) (2.10.0)\n",
      "Requirement already satisfied: absl-py~=0.10 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from tensorflow) (0.12.0)\n",
      "Requirement already satisfied: keras-preprocessing~=1.1.2 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from tensorflow) (1.1.2)\n",
      "Requirement already satisfied: termcolor~=1.1.0 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from tensorflow) (1.1.0)\n",
      "Requirement already satisfied: flatbuffers~=1.12.0 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from tensorflow) (1.12)\n",
      "Requirement already satisfied: setuptools in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from protobuf>=3.9.2->tensorflow) (41.2.0)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from tensorboard~=2.4->tensorflow) (1.8.0)\n",
      "Requirement already satisfied: google-auth<2,>=1.6.3 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from tensorboard~=2.4->tensorflow) (1.23.0)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from tensorboard~=2.4->tensorflow) (2.24.0)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from tensorboard~=2.4->tensorflow) (1.0.1)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from tensorboard~=2.4->tensorflow) (0.4.2)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from tensorboard~=2.4->tensorflow) (3.3.4)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow) (4.1.1)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.5\" in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow) (4.6)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow) (2020.6.20)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow) (3.0.4)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow) (1.25.10)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: idna<3,>=2.5 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow) (2.10)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow) (1.3.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow) (3.1.0)\n",
      "\u001b[33mWARNING: You are using pip version 20.1; however, version 21.0.1 is available.\n",
      "You should consider upgrading via the '/usr/local/bin/python3.8 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Looking in indexes: https://pypi.org/simple, https://pip:****@pypi.infra.unbabel.com/simple/\n",
      "Requirement already satisfied: pydot in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (1.4.2)\n",
      "Requirement already satisfied: pyparsing>=2.1.4 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from pydot) (2.4.7)\n",
      "\u001b[33mWARNING: You are using pip version 20.1; however, version 21.0.1 is available.\n",
      "You should consider upgrading via the '/usr/local/bin/python3.8 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Looking in indexes: https://pypi.org/simple, https://pip:****@pypi.infra.unbabel.com/simple/\n",
      "Requirement already satisfied: graphviz in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (0.16)\n",
      "\u001b[33mWARNING: You are using pip version 20.1; however, version 21.0.1 is available.\n",
      "You should consider upgrading via the '/usr/local/bin/python3.8 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Looking in indexes: https://pypi.org/simple, https://pip:****@pypi.infra.unbabel.com/simple/\n",
      "Requirement already satisfied: hunga-bunga in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (0.1)\n",
      "Requirement already satisfied: numpy in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from hunga-bunga) (1.19.5)\n",
      "\u001b[33mWARNING: You are using pip version 20.1; however, version 21.0.1 is available.\n",
      "You should consider upgrading via the '/usr/local/bin/python3.8 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Looking in indexes: https://pypi.org/simple, https://pip:****@pypi.infra.unbabel.com/simple/\n",
      "Requirement already satisfied: tabulate in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (0.8.9)\n",
      "\u001b[33mWARNING: You are using pip version 20.1; however, version 21.0.1 is available.\n",
      "You should consider upgrading via the '/usr/local/bin/python3.8 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas\n",
    "!pip install matplotlib\n",
    "!pip install sklearn\n",
    "!pip install tensorflow\n",
    "!pip install pydot\n",
    "!pip install graphviz\n",
    "!pip install hunga-bunga\n",
    "!pip install tabulate\n",
    "\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "# Make numpy values easier to read.\n",
    "np.set_printoptions(precision=3, suppress=True)\n",
    "from matplotlib import pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers.experimental import preprocessing\n",
    "\n",
    "tf.random.set_seed(123)\n",
    "\n",
    "class CSV_FORMATS():\n",
    "  METRICS = \"metrics\" # dataset format with extracted metrics\n",
    "  OPERATIONS = \"operations\" # dataset format where every entry is an entity operation\n",
    "\n",
    "class TraceFeature():\n",
    "  def __init__(self, name: str, first_idx: int, last_idx: int =None):\n",
    "    self.name = name\n",
    "    self.first_idx = first_idx\n",
    "    self.last_idx = last_idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8vPQrp7iZ45S"
   },
   "source": [
    "# Read dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "id": "8WRiYi_KeYVl"
   },
   "outputs": [],
   "source": [
    "def read_dataset(file, row_names, rows_to_exclude):\n",
    "  return pd.read_csv(CSV_FILE, names=CSV_ROWS, skiprows=1, usecols = [i for i in CSV_ROWS if i not in CSV_ROWS_TO_EXCLUDE])\n",
    "\n",
    "\n",
    "def split_dataset_by_trace_features(trace_dataset):\n",
    "  features = []\n",
    "\n",
    "  for idx, name in enumerate(trace_dataset[\"Feature\"]):\n",
    "    current_feature = features[len(features) - 1] if len(features) > 0 else None\n",
    "\n",
    "    if current_feature is None or name != current_feature.name:\n",
    "      if len(features) > 0:\n",
    "        last_feature = features[len(features) - 1]\n",
    "        last_feature.last_idx = idx - 1\n",
    "      \n",
    "      features.append(TraceFeature(name, idx))\n",
    "\n",
    "  current_feature.last_idx = idx\n",
    "  return features\n",
    "\n",
    "\n",
    "def create_batch(dataset_features, dataset_labels, trace_controllers, string_to_id):\n",
    "  clusters_features = []\n",
    "  clusters_labels = []\n",
    "\n",
    "  for idx, controller in enumerate(trace_controllers):\n",
    "    clusters = []\n",
    "    for _ in range(controller.first_idx,controller.last_idx + 1):\n",
    "        clusters.append([])\n",
    "    \n",
    "    for feature, values in dataset_features.items():\n",
    "      for idx2, cluster_feature_value in enumerate(values[controller.first_idx:controller.last_idx + 1]):\n",
    "        if feature == \"Feature\":\n",
    "            feature_id = string_to_id.get(cluster_feature_value)\n",
    "            if not feature_id:\n",
    "                feature_id = string_to_id[\"next\"]\n",
    "                string_to_id[cluster_feature_value] = feature_id\n",
    "                string_to_id[\"next\"] += 1\n",
    "            \n",
    "            cluster_feature_value = feature_id\n",
    "    \n",
    "        clusters[idx2].append(cluster_feature_value)\n",
    "    \n",
    "    clusters_features = clusters_features + clusters\n",
    "    feature_labels = np.asarray(dataset_labels[controller.first_idx:controller.last_idx + 1]).astype('float32')\n",
    "\n",
    "    if idx == 0:\n",
    "      clusters_labels = feature_labels\n",
    "    else:\n",
    "      clusters_labels = np.concatenate((clusters_labels, feature_labels))\n",
    "  \n",
    "  clusters_features = np.array(clusters_features)\n",
    "  return clusters_features, clusters_labels\n",
    "\n",
    "\n",
    "def get_kfold_iteration_batches(\n",
    "    iteration,\n",
    "    dataset_features,\n",
    "    dataset_labels,\n",
    "    trace_features,\n",
    "    training_features_size,\n",
    "    validation_features_size,\n",
    "    testing_features_size\n",
    "):\n",
    "  testing_start_idx = iteration * testing_features_size\n",
    "  testing_end_idx = testing_start_idx + testing_features_size\n",
    "  testing_features = trace_features[testing_start_idx:testing_end_idx]\n",
    "\n",
    "  if iteration == 0:\n",
    "        training_start_idx = testing_end_idx + validation_features_size\n",
    "        training_features = trace_features[training_start_idx:]\n",
    "  elif iteration < (K_FOLD_VALUE - 1):\n",
    "        training_start_idx_2 = testing_end_idx + validation_features_size\n",
    "        training_features = trace_features[:testing_start_idx] + trace_features[training_start_idx_2:]\n",
    "  else:\n",
    "        training_features = trace_features[:testing_start_idx]\n",
    "\n",
    "  # now we divide the dataset into batches\n",
    "  string_to_id = {}\n",
    "  string_to_id[\"next\"] = 0\n",
    "  training_batch_features, training_batch_labels = create_batch(dataset_features, dataset_labels, training_features, string_to_id)\n",
    "  testing_batch_features, testing_batch_labels = create_batch(dataset_features, dataset_labels, testing_features, string_to_id)\n",
    "\n",
    "  validation_batch_features = None\n",
    "  validation_batch_labels = None\n",
    "  if APPLY_FIT_VALIDATION:\n",
    "    validation_end_idx = testing_end_idx + validation_features_size\n",
    "    validation_features = trace_features[testing_end_idx:validation_end_idx]\n",
    "    validation_batch_features, validation_batch_labels = create_batch(dataset_features, dataset_labels, validation_features, string_to_id)\n",
    "\n",
    "  return (training_batch_features, training_batch_labels), (testing_batch_features, testing_batch_labels), (validation_batch_features, validation_batch_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J-fq9mKZCFtF"
   },
   "source": [
    "# Preprocessing data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RFuNMGZMCwCv"
   },
   "source": [
    "To build the preprocessing model, start by building a set of symbolic keras.Input objects, matching the names and data-types of the CSV columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "hNOcBnv5CJKw"
   },
   "outputs": [],
   "source": [
    "def create_input_objects(dataset_features):\n",
    "  inputs = {}\n",
    "\n",
    "  for name, column in dataset_features.items():\n",
    "    dtype = column.dtype\n",
    "    if dtype == object:\n",
    "      dtype = tf.string\n",
    "    else:\n",
    "      dtype = tf.float32\n",
    "\n",
    "    inputs[name] = tf.keras.Input(shape=(1,), name=name, dtype=dtype)\n",
    "  \n",
    "  return inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "khDlQGeHCxhp"
   },
   "source": [
    "The first step in the preprocessing logic is to concatenate the numeric inputs together, and run them through a normalization layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "ebbp60ruC0z1"
   },
   "outputs": [],
   "source": [
    "def create_preprocessing_logic(dataset, dataset_features, inputs):\n",
    "  numeric_inputs = {name:input for name,input in inputs.items()\n",
    "                    if input.dtype==tf.float32}\n",
    "\n",
    "  preprocessed_inputs = []\n",
    "  if numeric_inputs:\n",
    "    x = layers.Concatenate()(list(numeric_inputs.values()))\n",
    "    norm = preprocessing.Normalization()\n",
    "    norm.adapt(np.array(dataset[numeric_inputs.keys()]))\n",
    "    all_numeric_inputs = norm(x)\n",
    "\n",
    "    # Collect all the symbolic preprocessing results, to concatenate them later.\n",
    "    preprocessed_inputs = [all_numeric_inputs]\n",
    "\n",
    "    # For the string inputs use the preprocessing.StringLookup function to map from \n",
    "    # strings to integer indices in a vocabulary. Next, use preprocessing.CategoryEncoding \n",
    "    # to convert the indexes into float32 data appropriate for the model.\n",
    "    for name, input in inputs.items():\n",
    "      if input.dtype == tf.float32:\n",
    "        continue\n",
    "\n",
    "      lookup = preprocessing.StringLookup(vocabulary=np.unique(dataset_features[name]))\n",
    "      one_hot = preprocessing.CategoryEncoding(max_tokens=lookup.vocab_size())\n",
    "\n",
    "      x = lookup(input)\n",
    "      x = one_hot(x)\n",
    "      preprocessed_inputs.append(x)\n",
    "    \n",
    "  preprocessed_inputs_cat = layers.Concatenate()(preprocessed_inputs)\n",
    "  preprocessing_model = tf.keras.Model(inputs, preprocessed_inputs_cat)\n",
    "  tf.keras.utils.plot_model(model = preprocessing_model , rankdir=\"LR\", dpi=126, show_shapes=True)\n",
    "  \n",
    "  return preprocessing_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-_nTiD3nCJnB"
   },
   "source": [
    "# Design Neural Network model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ktPogffkE0aj"
   },
   "source": [
    "Now build the model on top of this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "LNDJU8ncE1Bu"
   },
   "outputs": [],
   "source": [
    "def build_neural_network_model(body, preprocessing_head, inputs, loss, optimizer):\n",
    "  preprocessed_inputs = preprocessing_head(inputs)\n",
    "  result = tf.keras.Sequential(body)(preprocessed_inputs)\n",
    "  model = tf.keras.Model(inputs, result)\n",
    "\n",
    "  # The purpose of loss functions is to compute the quantity that \n",
    "  # a model should seek to minimize during training.\n",
    "  # Binary classification loss function comes into play when solving a problem \n",
    "  # involving just two classes (1 or 0)\n",
    "\n",
    "  # Adam optimization is a stochastic gradient descent method that is based on \n",
    "  # adaptive estimation of first-order and second-order moments.\n",
    "  # is computationally efficient, has little memory requirement, and is well \n",
    "  # suited for problems that are large in terms of data/parameters\"\n",
    "  model.compile(\n",
    "      loss=loss,\n",
    "      optimizer=optimizer,  \n",
    "      metrics=[\"accuracy\"],  \n",
    "  )\n",
    "  return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8S8FEyx-j3fL"
   },
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "C9uXb2Krfffe"
   },
   "outputs": [],
   "source": [
    "def fit_neural_network(model, training_features, training_labels, validation_features, validation_labels, epochs, shuffle, weights):\n",
    "  callbacks = [\n",
    "      tf.keras.callbacks.EarlyStopping(\n",
    "          # Stop training when `loss` is no longer improving\n",
    "          monitor=\"loss\",\n",
    "          # \"no longer improving\" being defined as \"no better than 1e-2 less\"\n",
    "          min_delta=1e-4,\n",
    "          # \"no longer improving\" being further defined as \"for at least 2 epochs\"\n",
    "          patience=2,\n",
    "          verbose=1,\n",
    "      )\n",
    "  ]\n",
    "\n",
    "  history = model.fit(\n",
    "      x=training_features,\n",
    "      y=training_labels,\n",
    "      callbacks=callbacks,\n",
    "      shuffle=shuffle,\n",
    "      epochs=epochs,\n",
    "      validation_data=(validation_features, validation_labels) if APPLY_FIT_VALIDATION else None,\n",
    "      class_weight=weights, # This argument allows you to define a dictionary that maps class integer values to the importance to apply to each class.\n",
    "      verbose=0,\n",
    "  )\n",
    "\n",
    "  return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "SrM0S1ooTePk"
   },
   "outputs": [],
   "source": [
    "def plot_training_results(history):\n",
    "  # plot loss during training\n",
    "  plt.figure(1)\n",
    "  plt.title('Loss')\n",
    "  plt.plot(history.history['loss'], label='train')\n",
    "\n",
    "  if APPLY_FIT_VALIDATION:\n",
    "    plt.plot(history.history['val_loss'], label='test')\n",
    "\n",
    "  plt.legend()\n",
    "\n",
    "  # plot accuracy during training\n",
    "  plt.figure(2)\n",
    "  plt.title('Accuracy')\n",
    "  plt.plot(history.history['accuracy'], label='train')\n",
    "\n",
    "  if APPLY_FIT_VALIDATION:\n",
    "    plt.plot(history.history['val_accuracy'], label='test')\n",
    "\n",
    "  plt.legend()\n",
    "  plt.show()\n",
    "\n",
    "  print(f\"\\nTraining results:\\nFinal loss: {history.history['loss'][len(history.history['loss'])-1]}\")\n",
    "  print(f\"Final accuracy: {history.history['accuracy'][len(history.history['accuracy'])-1]}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NSPLA7dEkFNK"
   },
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "oXETbNmBd63F"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "def test_model(model, testing_features, testing_labels, verbose=True):\n",
    "  print(f\"Results for {testing_labels.size} test samples\\n\")\n",
    "\n",
    "  results = model.evaluate(testing_features, testing_labels, batch_size=testing_labels.size, verbose=0)\n",
    "  print(f\"Loss {results[0]} | Recall: {results[1]}\\n\")\n",
    "\n",
    "  predictions = model.predict(testing_features)\n",
    "\n",
    "  if verbose:\n",
    "    for idx, prediction in enumerate(predictions):\n",
    "      label = 0 if prediction[0] > 0.500 else 1\n",
    "      percentage = prediction[0] if label == 0 else prediction[1]\n",
    "      percentage = int(percentage * 100)\n",
    "      correct_label = testing_labels[idx]\n",
    "      feature = testing_features[\"Feature\"][idx]\n",
    "      \n",
    "      print(f\"Prediction: {label} ({percentage} %) | Correct: {correct_label} | Feature: {feature}\")\n",
    "  \n",
    "  return predictions, results[0], results[1]\n",
    "\n",
    "# !!!!!!!!!!!!!!!!!!\n",
    "#  If one feature has multiple clusters being the orchestrator, we should select the one with\n",
    "#  the highest probability\n",
    "\n",
    "# evaluate the ROC AUC of the predictions\n",
    "def plot_testing_results(predictions, testing_labels):\n",
    "  results = []\n",
    "  for prediction in predictions:\n",
    "    label = 0 if prediction[0] > 0.5 else 1\n",
    "    results.append(label)\n",
    "  \n",
    "  fpr_keras, tpr_keras, thresholds_keras = roc_curve(testing_labels, results)\n",
    "  \n",
    "  auc_keras = auc(fpr_keras, tpr_keras)\n",
    "\n",
    "  print(\"\\n\")\n",
    "  plt.figure(1)\n",
    "  plt.plot([0, 1], [0, 1], 'k--')\n",
    "  plt.plot(fpr_keras, tpr_keras, label='Keras (area = {:.3f})'.format(auc_keras))\n",
    "  plt.xlabel('False positive rate')\n",
    "  plt.ylabel('True positive rate')\n",
    "  plt.title('ROC curve')\n",
    "  plt.legend(loc='best')\n",
    "  plt.show()\n",
    "\n",
    "  print(f\"AUC: {auc_keras}\")\n",
    "  return auc_keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RJDuvJ1sG85z"
   },
   "source": [
    "# Main script execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "kUumMKQ-G_g-",
    "outputId": "2783fbf2-f640-463e-b431-fe3beeb73a8f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Failed to import pydot. You must `pip install pydot` and install graphviz (https://graphviz.gitlab.io/download/), ', 'for `pydotprint` to work.')\n",
      "\n",
      "\n",
      "Batch size: 2325 | Number of trace features: 717\n",
      "Training size: 645 | Validation size: 0 | Testing size: 72\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 9/15 [03:48<02:32, 25.40s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-80-4879f449736e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m   \u001b[0mclf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mHungaBungaClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbrain\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m   clf.fit(\n\u001b[0m\u001b[1;32m     87\u001b[0m       \u001b[0mtraining_batch_features\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m       \u001b[0mtraining_batch_labels\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/hunga_bunga/classification.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y)\u001b[0m\n\u001b[1;32m    196\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 198\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_all_classifiers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnormalize_x\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_splits\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_splits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mupsample\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupsample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscoring\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscoring\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbrain\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrid_search\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrid_search\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    199\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/hunga_bunga/classification.py\u001b[0m in \u001b[0;36mrun_all_classifiers\u001b[0;34m(x, y, small, normalize_x, n_jobs, brain, test_size, n_splits, upsample, scoring, verbose, grid_search)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mrun_all_classifiers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msmall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnormalize_x\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcpu_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbrain\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_splits\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mupsample\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscoring\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrid_search\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m     \u001b[0mall_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlinear_models_n_params_small\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0msmall\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mlinear_models_n_params\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m  \u001b[0;34m(\u001b[0m\u001b[0mnn_models_n_params_small\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0msmall\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mnn_models_n_params\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0msmall\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mgaussianprocess_models_n_params\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mneighbor_models_n_params\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msvm_models_n_params_small\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0msmall\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0msvm_models_n_params\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtree_models_n_params_small\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0msmall\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mtree_models_n_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 179\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mmain_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mStandardScaler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mnormalize_x\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0misClassification\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbrain\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtest_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_splits\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_splits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mupsample\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mupsample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscoring\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscoring\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrid_search\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgrid_search\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    180\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/hunga_bunga/core.py\u001b[0m in \u001b[0;36mmain_loop\u001b[0;34m(models_n_params, x, y, isClassification, test_size, n_splits, random_state, upsample, scoring, verbose, n_jobs, brain, grid_search)\u001b[0m\n\u001b[1;32m    103\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mgrid_search\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mclf_search\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGridSearchCVProgressBar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclf_Klass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscoring\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcv_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mclf_search\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRandomizedSearchCVProgressBar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclf_Klass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscoring\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcv_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 105\u001b[0;31m             \u001b[0mclf_search\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    106\u001b[0m             \u001b[0mtimespent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtimeit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclf_Klass\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclf_search\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_params_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbrain\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'best score:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclf_search\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_score_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'time/clf: %0.3f seconds'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mtimespent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0mextra_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mextra_args\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0;31m# extra_args > 0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    839\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    840\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 841\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    842\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    843\u001b[0m             \u001b[0;31m# multimetric is determined here because in the case of a callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36m_run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1286\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1287\u001b[0m         \u001b[0;34m\"\"\"Search all candidates in param_grid\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1288\u001b[0;31m         \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mParameterGrid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1289\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1290\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[0;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[1;32m    793\u001b[0m                               n_splits, n_candidates, n_candidates * n_splits))\n\u001b[1;32m    794\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 795\u001b[0;31m                 out = parallel(delayed(_fit_and_score)(clone(base_estimator),\n\u001b[0m\u001b[1;32m    796\u001b[0m                                                        \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    797\u001b[0m                                                        \u001b[0mtrain\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1052\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieval_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1054\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1055\u001b[0m             \u001b[0;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1056\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    931\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    932\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'supports_timeout'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 933\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    934\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    935\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mwrap_future_result\u001b[0;34m(future, timeout)\u001b[0m\n\u001b[1;32m    540\u001b[0m         AsyncResults.get from multiprocessing.\"\"\"\n\u001b[1;32m    541\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 542\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfuture\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    543\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mCfTimeoutError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    544\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/concurrent/futures/_base.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    432\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    433\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 434\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_condition\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    435\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    436\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mCANCELLED\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCANCELLED_AND_NOTIFIED\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    300\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    301\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 302\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    303\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    304\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# -------------------------------------------------------------\n",
    "# SCRIPT CONFIGURATION\n",
    "# -------------------------------------------------------------\n",
    "CSV_FORMAT = CSV_FORMATS.METRICS\n",
    "\n",
    "NETWORK_ARCHITECTURE = [\n",
    "    layers.Dense(9, activation=\"relu\"),\n",
    "    layers.Dense(16, activation=\"relu\"),\n",
    "    layers.Dense(2, activation=\"softmax\")\n",
    "]\n",
    "\n",
    "TRAINING_EPOCHS = 100\n",
    "LEARNING_RATE = 0.0001\n",
    "SHUFFLE_ON_TRAINING = False\n",
    "CLASS_WEIGHTS = {0:1, 1:3}\n",
    "\n",
    "APPLY_FIT_VALIDATION = False\n",
    "K_FOLD_VALUE = 1\n",
    "\n",
    "EXPORT_MODEL = False\n",
    "\n",
    "# -------------------------------------------------------------\n",
    "# EXECUTION\n",
    "# -------------------------------------------------------------\n",
    "if CSV_FORMAT == CSV_FORMATS.METRICS:\n",
    "  CSV_FILE = \"../output/ml-dataset-23-03.csv\"\n",
    "  CSV_ROWS = [\"Codebase\", \"Feature\", \"Cluster\", \"CLIP\", \"CRIP\", \"CROP\", \"CWOP\", \"CIP\", \"COP\", \"CPIF\", \"CIOF\", \"Orchestrator\"]\n",
    "  CSV_ROWS_TO_EXCLUDE = [\"Cluster\", \"Codebase\", \"CROP\", \"CWOP\", \"CLIP\", \"CRIP\"]\n",
    "  #CSV_ROWS_TO_EXCLUDE = []\n",
    "\n",
    "elif CSV_FORMAT == CSV_FORMATS.OPERATIONS:\n",
    "  CSV_FILE = \"2021-03-16 23:41:19.csv\"\n",
    "  CSV_ROWS = [\"Codebase\", \"Feature\", \"Cluster\", \"Entity\", \"Operation\", \"Orchestrator\"]\n",
    "  #CSV_ROWS_TO_EXCLUDE = [\"Codebase\"]\n",
    "  CSV_ROWS_TO_EXCLUDE = [\"Cluster\"]\n",
    "\n",
    "\n",
    "dataset = read_dataset(CSV_FILE, CSV_ROWS, CSV_ROWS_TO_EXCLUDE)\n",
    "# print(dataset.head())\n",
    "\n",
    "dataset_features = dataset.copy()\n",
    "dataset_labels = dataset_features.pop('Orchestrator')\n",
    "\n",
    "# generate a trace_features array to make the splitting of the batches easier\n",
    "trace_features = split_dataset_by_trace_features(dataset)\n",
    "random.shuffle(trace_features)\n",
    "\n",
    "# preprocessing\n",
    "inputs = create_input_objects(dataset_features)\n",
    "trace_preprocessing = create_preprocessing_logic(dataset, dataset_features, inputs)\n",
    "\n",
    "number_trace_features = len(trace_features)\n",
    "if K_FOLD_VALUE == 1:\n",
    "    training_features_size = int(number_trace_features - (number_trace_features*0.1))\n",
    "else:\n",
    "    training_features_size = int(number_trace_features - (number_trace_features/K_FOLD_VALUE))\n",
    "validation_features_size = int((number_trace_features - training_features_size) / 2) if APPLY_FIT_VALIDATION else 0\n",
    "testing_features_size = number_trace_features - training_features_size - validation_features_size\n",
    "\n",
    "print(f\"\\n\\nBatch size: {dataset_labels.size} | Number of trace features: {number_trace_features}\")\n",
    "print(f\"Training size: {training_features_size} | Validation size: {validation_features_size} | Testing size: {testing_features_size}\\n\\n\")\n",
    "\n",
    "\n",
    "histories = []\n",
    "labels = []\n",
    "predictions = []\n",
    "aucs = []\n",
    "losses = []\n",
    "recalls = []\n",
    "\n",
    "from hunga_bunga import HungaBungaClassifier, HungaBungaRegressor\n",
    "\n",
    "for iteration in range(K_FOLD_VALUE):\n",
    "  (training_batch_features, training_batch_labels), (testing_batch_features, testing_batch_labels), (validation_batch_features, validation_batch_labels) = get_kfold_iteration_batches(\n",
    "      iteration,\n",
    "      dataset_features,\n",
    "      dataset_labels,\n",
    "      trace_features,\n",
    "      training_features_size,\n",
    "      validation_features_size,\n",
    "      testing_features_size,\n",
    "  )\n",
    "  labels.append(testing_batch_labels)\n",
    "\n",
    "  clf = HungaBungaClassifier(brain=False)\n",
    "  clf.fit(\n",
    "      training_batch_features,\n",
    "      training_batch_labels,\n",
    "  )\n",
    "\n",
    "  print(\"\\n\\n--------------------------------------------------------------------------------------\\n\\n\")\n",
    "\n",
    "\n",
    "if EXPORT_MODEL:\n",
    "  filename = f'trace_trained_model-{datetime.now().strftime(\"%d_%m_%Y_%H_%M_%S\")}'\n",
    "  model.save(filename)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "trace_neural_network.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
