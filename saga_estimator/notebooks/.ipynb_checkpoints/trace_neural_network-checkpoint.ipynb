{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6DhUzc1JpEhS"
   },
   "source": [
    "# Script configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "id": "gusVhoCtopNm"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pip:****@pypi.infra.unbabel.com/simple/\n",
      "Requirement already satisfied: pandas in ./venv/lib/python3.8/site-packages (1.2.3)\n",
      "Requirement already satisfied: numpy>=1.16.5 in ./venv/lib/python3.8/site-packages (from pandas) (1.19.5)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in ./venv/lib/python3.8/site-packages (from pandas) (2.8.1)\n",
      "Requirement already satisfied: pytz>=2017.3 in ./venv/lib/python3.8/site-packages (from pandas) (2021.1)\n",
      "Requirement already satisfied: six>=1.5 in ./venv/lib/python3.8/site-packages (from python-dateutil>=2.7.3->pandas) (1.15.0)\n",
      "\u001b[33mWARNING: You are using pip version 21.0.1; however, version 21.1 is available.\n",
      "You should consider upgrading via the '/Users/josecorreia/Desktop/tese/automation/saga_estimator/notebooks/venv/bin/python -m pip install --upgrade pip' command.\u001b[0m\n",
      "Looking in indexes: https://pypi.org/simple, https://pip:****@pypi.infra.unbabel.com/simple/\n",
      "Requirement already satisfied: matplotlib in ./venv/lib/python3.8/site-packages (3.4.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in ./venv/lib/python3.8/site-packages (from matplotlib) (2.8.1)\n",
      "Requirement already satisfied: numpy>=1.16 in ./venv/lib/python3.8/site-packages (from matplotlib) (1.19.5)\n",
      "Requirement already satisfied: pillow>=6.2.0 in ./venv/lib/python3.8/site-packages (from matplotlib) (8.1.2)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in ./venv/lib/python3.8/site-packages (from matplotlib) (1.3.1)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in ./venv/lib/python3.8/site-packages (from matplotlib) (2.4.7)\n",
      "Requirement already satisfied: cycler>=0.10 in ./venv/lib/python3.8/site-packages (from matplotlib) (0.10.0)\n",
      "Requirement already satisfied: six in ./venv/lib/python3.8/site-packages (from cycler>=0.10->matplotlib) (1.15.0)\n",
      "\u001b[33mWARNING: You are using pip version 21.0.1; however, version 21.1 is available.\n",
      "You should consider upgrading via the '/Users/josecorreia/Desktop/tese/automation/saga_estimator/notebooks/venv/bin/python -m pip install --upgrade pip' command.\u001b[0m\n",
      "Looking in indexes: https://pypi.org/simple, https://pip:****@pypi.infra.unbabel.com/simple/\n",
      "Requirement already satisfied: sklearn in ./venv/lib/python3.8/site-packages (0.0)\n",
      "Requirement already satisfied: scikit-learn in ./venv/lib/python3.8/site-packages (from sklearn) (0.24.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in ./venv/lib/python3.8/site-packages (from scikit-learn->sklearn) (2.1.0)\n",
      "Requirement already satisfied: joblib>=0.11 in ./venv/lib/python3.8/site-packages (from scikit-learn->sklearn) (1.0.1)\n",
      "Requirement already satisfied: numpy>=1.13.3 in ./venv/lib/python3.8/site-packages (from scikit-learn->sklearn) (1.19.5)\n",
      "Requirement already satisfied: scipy>=0.19.1 in ./venv/lib/python3.8/site-packages (from scikit-learn->sklearn) (1.6.2)\n",
      "\u001b[33mWARNING: You are using pip version 21.0.1; however, version 21.1 is available.\n",
      "You should consider upgrading via the '/Users/josecorreia/Desktop/tese/automation/saga_estimator/notebooks/venv/bin/python -m pip install --upgrade pip' command.\u001b[0m\n",
      "Looking in indexes: https://pypi.org/simple, https://pip:****@pypi.infra.unbabel.com/simple/\n",
      "Requirement already satisfied: tensorflow in ./venv/lib/python3.8/site-packages (2.4.1)\n",
      "Requirement already satisfied: h5py~=2.10.0 in ./venv/lib/python3.8/site-packages (from tensorflow) (2.10.0)\n",
      "Requirement already satisfied: keras-preprocessing~=1.1.2 in ./venv/lib/python3.8/site-packages (from tensorflow) (1.1.2)\n",
      "Requirement already satisfied: six~=1.15.0 in ./venv/lib/python3.8/site-packages (from tensorflow) (1.15.0)\n",
      "Requirement already satisfied: gast==0.3.3 in ./venv/lib/python3.8/site-packages (from tensorflow) (0.3.3)\n",
      "Requirement already satisfied: typing-extensions~=3.7.4 in ./venv/lib/python3.8/site-packages (from tensorflow) (3.7.4.3)\n",
      "Requirement already satisfied: termcolor~=1.1.0 in ./venv/lib/python3.8/site-packages (from tensorflow) (1.1.0)\n",
      "Requirement already satisfied: wrapt~=1.12.1 in ./venv/lib/python3.8/site-packages (from tensorflow) (1.12.1)\n",
      "Requirement already satisfied: wheel~=0.35 in ./venv/lib/python3.8/site-packages (from tensorflow) (0.36.2)\n",
      "Requirement already satisfied: google-pasta~=0.2 in ./venv/lib/python3.8/site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: absl-py~=0.10 in ./venv/lib/python3.8/site-packages (from tensorflow) (0.12.0)\n",
      "Requirement already satisfied: astunparse~=1.6.3 in ./venv/lib/python3.8/site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: tensorboard~=2.4 in ./venv/lib/python3.8/site-packages (from tensorflow) (2.4.1)\n",
      "Requirement already satisfied: grpcio~=1.32.0 in ./venv/lib/python3.8/site-packages (from tensorflow) (1.32.0)\n",
      "Requirement already satisfied: opt-einsum~=3.3.0 in ./venv/lib/python3.8/site-packages (from tensorflow) (3.3.0)\n",
      "Requirement already satisfied: tensorflow-estimator<2.5.0,>=2.4.0 in ./venv/lib/python3.8/site-packages (from tensorflow) (2.4.0)\n",
      "Requirement already satisfied: flatbuffers~=1.12.0 in ./venv/lib/python3.8/site-packages (from tensorflow) (1.12)\n",
      "Requirement already satisfied: numpy~=1.19.2 in ./venv/lib/python3.8/site-packages (from tensorflow) (1.19.5)\n",
      "Requirement already satisfied: protobuf>=3.9.2 in ./venv/lib/python3.8/site-packages (from tensorflow) (3.15.6)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in ./venv/lib/python3.8/site-packages (from tensorboard~=2.4->tensorflow) (53.1.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in ./venv/lib/python3.8/site-packages (from tensorboard~=2.4->tensorflow) (3.3.4)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in ./venv/lib/python3.8/site-packages (from tensorboard~=2.4->tensorflow) (1.8.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in ./venv/lib/python3.8/site-packages (from tensorboard~=2.4->tensorflow) (0.4.3)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in ./venv/lib/python3.8/site-packages (from tensorboard~=2.4->tensorflow) (1.0.1)\n",
      "Requirement already satisfied: google-auth<2,>=1.6.3 in ./venv/lib/python3.8/site-packages (from tensorboard~=2.4->tensorflow) (1.28.0)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in ./venv/lib/python3.8/site-packages (from tensorboard~=2.4->tensorflow) (2.25.1)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in ./venv/lib/python3.8/site-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in ./venv/lib/python3.8/site-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow) (4.7.2)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in ./venv/lib/python3.8/site-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow) (4.2.1)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in ./venv/lib/python3.8/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow) (1.3.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in ./venv/lib/python3.8/site-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow) (0.4.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in ./venv/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow) (1.26.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./venv/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow) (2020.12.5)\n",
      "Requirement already satisfied: idna<3,>=2.5 in ./venv/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow) (2.10)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in ./venv/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow) (4.0.0)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in ./venv/lib/python3.8/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow) (3.1.0)\n",
      "\u001b[33mWARNING: You are using pip version 21.0.1; however, version 21.1 is available.\n",
      "You should consider upgrading via the '/Users/josecorreia/Desktop/tese/automation/saga_estimator/notebooks/venv/bin/python -m pip install --upgrade pip' command.\u001b[0m\n",
      "Looking in indexes: https://pypi.org/simple, https://pip:****@pypi.infra.unbabel.com/simple/\n",
      "Requirement already satisfied: pydot in ./venv/lib/python3.8/site-packages (1.4.2)\n",
      "Requirement already satisfied: pyparsing>=2.1.4 in ./venv/lib/python3.8/site-packages (from pydot) (2.4.7)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: You are using pip version 21.0.1; however, version 21.1 is available.\n",
      "You should consider upgrading via the '/Users/josecorreia/Desktop/tese/automation/saga_estimator/notebooks/venv/bin/python -m pip install --upgrade pip' command.\u001b[0m\n",
      "Looking in indexes: https://pypi.org/simple, https://pip:****@pypi.infra.unbabel.com/simple/\n",
      "Requirement already satisfied: graphviz in ./venv/lib/python3.8/site-packages (0.16)\n",
      "\u001b[33mWARNING: You are using pip version 21.0.1; however, version 21.1 is available.\n",
      "You should consider upgrading via the '/Users/josecorreia/Desktop/tese/automation/saga_estimator/notebooks/venv/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas\n",
    "!pip install matplotlib\n",
    "!pip install sklearn\n",
    "!pip install tensorflow\n",
    "!pip install pydot\n",
    "!pip install graphviz\n",
    "\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "# Make numpy values easier to read.\n",
    "np.set_printoptions(precision=3, suppress=True)\n",
    "from matplotlib import pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers.experimental import preprocessing\n",
    "\n",
    "tf.random.set_seed(123)\n",
    "\n",
    "class CSV_FORMATS():\n",
    "  METRICS = \"metrics\" # dataset format with extracted metrics\n",
    "  OPERATIONS = \"operations\" # dataset format where every entry is an entity operation\n",
    "\n",
    "class TraceFeature():\n",
    "  def __init__(self, codebase: str, name: str, first_idx: int, last_idx: int =None):\n",
    "    self.codebase = codebase\n",
    "    self.name = name\n",
    "    self.first_idx = first_idx\n",
    "    self.last_idx = last_idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8vPQrp7iZ45S"
   },
   "source": [
    "# Read dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "id": "8WRiYi_KeYVl"
   },
   "outputs": [],
   "source": [
    "def read_dataset(file, row_names, rows_to_exclude):\n",
    "  return pd.read_csv(file, names=row_names, skiprows=1, usecols = [i for i in row_names if i not in rows_to_exclude])\n",
    "\n",
    "\n",
    "def split_dataset_by_trace_features(trace_dataset):\n",
    "  features = []\n",
    "\n",
    "  for idx, name in enumerate(trace_dataset[\"Feature\"]):\n",
    "    current_feature = features[len(features) - 1] if len(features) > 0 else None\n",
    "\n",
    "    if current_feature is None or name != current_feature.name:\n",
    "      if len(features) > 0:\n",
    "        last_feature = features[len(features) - 1]\n",
    "        last_feature.last_idx = idx - 1\n",
    "      \n",
    "      codebase = trace_dataset[\"Codebase\"][idx]\n",
    "      features.append(TraceFeature(codebase, name, idx))\n",
    "\n",
    "  current_feature.last_idx = idx\n",
    "  return features\n",
    "\n",
    "\n",
    "def get_empty_features_dict(dataset_features):\n",
    "  return {name:[] for name, _ in dataset_features.items()}\n",
    "\n",
    "\n",
    "def create_batch(dataset_features, dataset_labels, trace_features):\n",
    "  batch_features = get_empty_features_dict(dataset_features)\n",
    "  batch_labels = None\n",
    "\n",
    "  for idx, feature in enumerate(trace_features):\n",
    "    for key, values in dataset_features.items():\n",
    "      feature_values = values[feature.first_idx:feature.last_idx + 1]\n",
    "      batch_features[key] = np.concatenate((batch_features[key], feature_values))\n",
    "    \n",
    "    feature_labels = np.asarray(dataset_labels[feature.first_idx:feature.last_idx + 1]).astype('float32').reshape((-1,1))\n",
    "\n",
    "    if idx == 0:\n",
    "      batch_labels = feature_labels\n",
    "    else:\n",
    "      batch_labels = np.concatenate((batch_labels, feature_labels))\n",
    "\n",
    "  return batch_features, batch_labels\n",
    "\n",
    "\n",
    "def get_kfold_iteration_batches(\n",
    "    iteration,\n",
    "    dataset_features,\n",
    "    dataset_labels,\n",
    "    trace_features,\n",
    "    training_features_size,\n",
    "    validation_features_size,\n",
    "    testing_features_size\n",
    "):\n",
    "  testing_features = []\n",
    "  training_features = []\n",
    "  if TEST_ON_SPECIFIC_CODEBASE:\n",
    "    for feature in trace_features:\n",
    "        if feature.codebase == TEST_ON_SPECIFIC_CODEBASE:\n",
    "            testing_features.append(feature)\n",
    "        else:\n",
    "            training_features.append(feature)\n",
    "\n",
    "  else:\n",
    "      testing_start_idx = iteration * testing_features_size\n",
    "      testing_end_idx = testing_start_idx + testing_features_size\n",
    "      testing_features = trace_features[testing_start_idx:testing_end_idx]\n",
    "\n",
    "      if iteration == 0:\n",
    "            training_start_idx = testing_end_idx + validation_features_size\n",
    "            training_features = trace_features[training_start_idx:]\n",
    "      elif iteration < (K_FOLD_VALUE - 1):\n",
    "            training_start_idx_2 = testing_end_idx + validation_features_size\n",
    "            training_features = trace_features[:testing_start_idx] + trace_features[training_start_idx_2:]\n",
    "      else:\n",
    "            training_features = trace_features[:testing_start_idx]\n",
    "\n",
    "  # now we divide the dataset into batches\n",
    "  training_batch_features, training_batch_labels = create_batch(dataset_features, dataset_labels, training_features)\n",
    "  testing_batch_features, testing_batch_labels = create_batch(dataset_features, dataset_labels, testing_features)\n",
    "\n",
    "  validation_batch_features = None\n",
    "  validation_batch_labels = None\n",
    "  if APPLY_FIT_VALIDATION:\n",
    "    validation_end_idx = testing_end_idx + validation_features_size\n",
    "    validation_features = trace_features[testing_end_idx:validation_end_idx]\n",
    "    validation_batch_features, validation_batch_labels = create_batch(dataset_features, dataset_labels, validation_features)\n",
    "\n",
    "  return (training_batch_features, training_batch_labels), (testing_batch_features, testing_batch_labels), (validation_batch_features, validation_batch_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J-fq9mKZCFtF"
   },
   "source": [
    "# Preprocessing data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RFuNMGZMCwCv"
   },
   "source": [
    "To build the preprocessing model, start by building a set of symbolic keras.Input objects, matching the names and data-types of the CSV columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "id": "hNOcBnv5CJKw"
   },
   "outputs": [],
   "source": [
    "def create_input_objects(dataset_features):\n",
    "  inputs = {}\n",
    "\n",
    "  for name, column in dataset_features.items():\n",
    "    dtype = column.dtype\n",
    "    if dtype == object:\n",
    "      dtype = tf.string\n",
    "    else:\n",
    "      dtype = tf.float32\n",
    "\n",
    "    inputs[name] = tf.keras.Input(shape=(1,), name=name, dtype=dtype)\n",
    "  \n",
    "  return inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "khDlQGeHCxhp"
   },
   "source": [
    "The first step in the preprocessing logic is to concatenate the numeric inputs together, and run them through a normalization layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "id": "ebbp60ruC0z1"
   },
   "outputs": [],
   "source": [
    "def create_preprocessing_logic(dataset, dataset_features, inputs):\n",
    "  numeric_inputs = {name:input for name,input in inputs.items()\n",
    "                    if input.dtype==tf.float32}\n",
    "\n",
    "  preprocessed_inputs = []\n",
    "  if numeric_inputs:\n",
    "    x = layers.Concatenate()(list(numeric_inputs.values()))\n",
    "    norm = preprocessing.Normalization()\n",
    "    norm.adapt(np.array(dataset[numeric_inputs.keys()]))\n",
    "    all_numeric_inputs = norm(x)\n",
    "\n",
    "    # Collect all the symbolic preprocessing results, to concatenate them later.\n",
    "    preprocessed_inputs = [all_numeric_inputs]\n",
    "\n",
    "    # For the string inputs use the preprocessing.StringLookup function to map from \n",
    "    # strings to integer indices in a vocabulary. Next, use preprocessing.CategoryEncoding \n",
    "    # to convert the indexes into float32 data appropriate for the model.\n",
    "    for name, input in inputs.items():\n",
    "      if input.dtype == tf.float32:\n",
    "        continue\n",
    "\n",
    "      lookup = preprocessing.StringLookup(vocabulary=np.unique(dataset_features[name]))\n",
    "      one_hot = preprocessing.CategoryEncoding(max_tokens=lookup.vocab_size())\n",
    "\n",
    "      x = lookup(input)\n",
    "      x = one_hot(x)\n",
    "      preprocessed_inputs.append(x)\n",
    "    \n",
    "  preprocessed_inputs_cat = layers.Concatenate()(preprocessed_inputs)\n",
    "  preprocessing_model = tf.keras.Model(inputs, preprocessed_inputs_cat)\n",
    "  tf.keras.utils.plot_model(model = preprocessing_model , rankdir=\"LR\", dpi=126, show_shapes=True)\n",
    "  \n",
    "  return preprocessing_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-_nTiD3nCJnB"
   },
   "source": [
    "# Design Neural Network model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ktPogffkE0aj"
   },
   "source": [
    "Now build the model on top of this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "id": "LNDJU8ncE1Bu"
   },
   "outputs": [],
   "source": [
    "def build_neural_network_model(body, preprocessing_head, inputs, loss, optimizer):\n",
    "  preprocessed_inputs = preprocessing_head(inputs)\n",
    "  result = tf.keras.Sequential(body)(preprocessed_inputs)\n",
    "  model = tf.keras.Model(inputs, result)\n",
    "\n",
    "  # The purpose of loss functions is to compute the quantity that \n",
    "  # a model should seek to minimize during training.\n",
    "  # Binary classification loss function comes into play when solving a problem \n",
    "  # involving just two classes (1 or 0)\n",
    "\n",
    "  # Adam optimization is a stochastic gradient descent method that is based on \n",
    "  # adaptive estimation of first-order and second-order moments.\n",
    "  # is computationally efficient, has little memory requirement, and is well \n",
    "  # suited for problems that are large in terms of data/parameters\"\n",
    "  model.compile(\n",
    "      loss=loss,\n",
    "      optimizer=optimizer,  \n",
    "      metrics=[\"accuracy\"],  \n",
    "  )\n",
    "  return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8S8FEyx-j3fL"
   },
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "id": "C9uXb2Krfffe"
   },
   "outputs": [],
   "source": [
    "def fit_neural_network(model, training_features, training_labels, validation_features, validation_labels, epochs, shuffle, weights):\n",
    "  callbacks = [\n",
    "      tf.keras.callbacks.EarlyStopping(\n",
    "          # Stop training when `loss` is no longer improving\n",
    "          monitor=\"loss\",\n",
    "          # \"no longer improving\" being defined as \"no better than 1e-2 less\"\n",
    "          min_delta=1e-4,\n",
    "          # \"no longer improving\" being further defined as \"for at least 2 epochs\"\n",
    "          patience=2,\n",
    "          verbose=1,\n",
    "      )\n",
    "  ]\n",
    "    \n",
    "  print(training_labels)\n",
    "  history = model.fit(\n",
    "      x=training_features,\n",
    "      y=training_labels,\n",
    "      callbacks=callbacks,\n",
    "      shuffle=shuffle,\n",
    "      epochs=epochs,\n",
    "      validation_data=(validation_features, validation_labels) if APPLY_FIT_VALIDATION else None,\n",
    "      class_weight=weights, # This argument allows you to define a dictionary that maps class integer values to the importance to apply to each class.\n",
    "      verbose=0,\n",
    "  )\n",
    "\n",
    "  return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "id": "SrM0S1ooTePk"
   },
   "outputs": [],
   "source": [
    "def plot_training_results(history):\n",
    "  # plot loss during training\n",
    "  plt.figure(1)\n",
    "  plt.title('Loss')\n",
    "  plt.plot(history.history['loss'], label='train')\n",
    "\n",
    "  if APPLY_FIT_VALIDATION:\n",
    "    plt.plot(history.history['val_loss'], label='test')\n",
    "\n",
    "  plt.legend()\n",
    "\n",
    "  # plot accuracy during training\n",
    "  plt.figure(2)\n",
    "  plt.title('Accuracy')\n",
    "  plt.plot(history.history['accuracy'], label='train')\n",
    "\n",
    "  if APPLY_FIT_VALIDATION:\n",
    "    plt.plot(history.history['val_accuracy'], label='test')\n",
    "\n",
    "  plt.legend()\n",
    "  plt.show()\n",
    "\n",
    "  print(f\"\\nTraining results:\\nFinal loss: {history.history['loss'][len(history.history['loss'])-1]}\")\n",
    "  print(f\"Final accuracy: {history.history['accuracy'][len(history.history['accuracy'])-1]}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NSPLA7dEkFNK"
   },
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "id": "oXETbNmBd63F"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "def test_model(model, testing_features, testing_labels, verbose=True):\n",
    "  print(f\"Results for {testing_labels.size} test samples\\n\")\n",
    "\n",
    "  results = model.evaluate(testing_features, testing_labels, batch_size=testing_labels.size, verbose=0)\n",
    "  print(f\"Loss {results[0]} | Recall: {results[1]}\\n\")\n",
    "\n",
    "  predictions = model.predict(testing_features)\n",
    "\n",
    "  if not verbose:\n",
    "    return predictions, results[0], results[1]\n",
    "\n",
    "  current_feature = None\n",
    "  feature_idxs = []\n",
    "  highest_score = 0.0\n",
    "  labels = []\n",
    "\n",
    "  for idx, prediction in enumerate(predictions):\n",
    "    feature = testing_features[\"Feature\"][idx]\n",
    "\n",
    "    label = 0 if prediction[0] > 0.500 else 1\n",
    "    labels.append(label)\n",
    "\n",
    "    percentage = prediction[0] if label == 0 else prediction[1]\n",
    "    percentage = int(percentage * 100)\n",
    "\n",
    "    if feature != current_feature:\n",
    "        current_feature = feature\n",
    "        feature_idxs = [idx]\n",
    "        highest_score = percentage\n",
    "    else:\n",
    "        feature_idxs.append(idx)\n",
    "\n",
    "\n",
    "    if percentage > highest_score and label == 1:\n",
    "        highest_score = percentage\n",
    "        for k in feature_idxs:\n",
    "            labels[k] = 0\n",
    "\n",
    "  for idx, prediction in enumerate(predictions):\n",
    "    correct_label = testing_labels[idx]\n",
    "    feature = testing_features[\"Feature\"][idx]\n",
    "    label = labels[idx]\n",
    "    percentage = prediction[0] if label == 0 else prediction[1]\n",
    "    print(f\"Prediction: {label} - {percentage}% | Correct: {correct_label} | Feature: {feature}\")\n",
    "\n",
    "  return predictions, results[0], results[1]\n",
    "\n",
    "# !!!!!!!!!!!!!!!!!!\n",
    "#  If one feature has multiple clusters being the orchestrator, we should select the one with\n",
    "#  the highest probability\n",
    "\n",
    "# evaluate the ROC AUC of the predictions\n",
    "def plot_testing_results(predictions, testing_labels):\n",
    "  results = []\n",
    "  for prediction in predictions:\n",
    "    label = 0 if prediction[0] > 0.5 else 1\n",
    "    results.append(label)\n",
    "    print\n",
    "    \n",
    "  \n",
    "  fpr_keras, tpr_keras, thresholds_keras = roc_curve(testing_labels, results)\n",
    "  \n",
    "  auc_keras = auc(fpr_keras, tpr_keras)\n",
    "\n",
    "  print(\"\\n\")\n",
    "  plt.figure(1)\n",
    "  plt.plot([0, 1], [0, 1], 'k--')\n",
    "  plt.plot(fpr_keras, tpr_keras, label='Keras (area = {:.3f})'.format(auc_keras))\n",
    "  plt.xlabel('False positive rate')\n",
    "  plt.ylabel('True positive rate')\n",
    "  plt.title('ROC curve')\n",
    "  plt.legend(loc='best')\n",
    "  plt.show()\n",
    "\n",
    "  print(f\"AUC: {auc_keras}\")\n",
    "  return auc_keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RJDuvJ1sG85z"
   },
   "source": [
    "# Main script execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "kUumMKQ-G_g-",
    "outputId": "2783fbf2-f640-463e-b431-fe3beeb73a8f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Failed to import pydot. You must `pip install pydot` and install graphviz (https://graphviz.gitlab.io/download/), ', 'for `pydotprint` to work.')\n",
      "\n",
      "\n",
      "Batch size: 111 | Number of trace features: 29\n",
      "Training size: 26 | Validation size: 1 | Testing size: 2\n",
      "\n",
      "\n",
      "[[0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py:805 train_function  *\n        return step_function(self, iterator)\n    /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py:795 step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/tensorflow/python/distribute/distribute_lib.py:1259 run\n        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\n    /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/tensorflow/python/distribute/distribute_lib.py:2730 call_for_each_replica\n        return self._call_for_each_replica(fn, args, kwargs)\n    /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/tensorflow/python/distribute/distribute_lib.py:3417 _call_for_each_replica\n        return fn(*args, **kwargs)\n    /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py:788 run_step  **\n        outputs = model.train_step(data)\n    /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py:755 train_step\n        loss = self.compiled_loss(\n    /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/tensorflow/python/keras/engine/compile_utils.py:203 __call__\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/tensorflow/python/keras/losses.py:152 __call__\n        losses = call_fn(y_true, y_pred)\n    /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/tensorflow/python/keras/losses.py:256 call  **\n        return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/tensorflow/python/util/dispatch.py:201 wrapper\n        return target(*args, **kwargs)\n    /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/tensorflow/python/keras/losses.py:1608 binary_crossentropy\n        K.binary_crossentropy(y_true, y_pred, from_logits=from_logits), axis=-1)\n    /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/tensorflow/python/util/dispatch.py:201 wrapper\n        return target(*args, **kwargs)\n    /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/tensorflow/python/keras/backend.py:4979 binary_crossentropy\n        return nn.sigmoid_cross_entropy_with_logits(labels=target, logits=output)\n    /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/tensorflow/python/util/dispatch.py:201 wrapper\n        return target(*args, **kwargs)\n    /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/tensorflow/python/ops/nn_impl.py:173 sigmoid_cross_entropy_with_logits\n        raise ValueError(\"logits and labels must have the same shape (%s vs %s)\" %\n\n    ValueError: logits and labels must have the same shape ((None, 2) vs (None, 1))\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-72-c52d47240973>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     83\u001b[0m   )\n\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m   training_history = fit_neural_network(\n\u001b[0m\u001b[1;32m     86\u001b[0m       \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m       \u001b[0mtraining_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtraining_batch_features\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-70-2c4be05adc74>\u001b[0m in \u001b[0;36mfit_neural_network\u001b[0;34m(model, training_features, training_labels, validation_features, validation_labels, epochs, shuffle, weights)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m   history = model.fit(\n\u001b[0m\u001b[1;32m     16\u001b[0m       \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining_features\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m       \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining_labels\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1098\u001b[0m                 _r=1):\n\u001b[1;32m   1099\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1100\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1101\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1102\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    826\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 828\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    829\u001b[0m       \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"xla\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_experimental_compile\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    830\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    869\u001b[0m       \u001b[0;31m# This is the first call of __call__, so we have to initialize.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    870\u001b[0m       \u001b[0minitializers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 871\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madd_initializers_to\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitializers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    872\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    873\u001b[0m       \u001b[0;31m# At this point we know that the initialization is complete (or less\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_initialize\u001b[0;34m(self, args, kwds, add_initializers_to)\u001b[0m\n\u001b[1;32m    723\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph_deleter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFunctionDeleter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lifted_initializer_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    724\u001b[0m     self._concrete_stateful_fn = (\n\u001b[0;32m--> 725\u001b[0;31m         self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m    726\u001b[0m             *args, **kwds))\n\u001b[1;32m    727\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_get_concrete_function_internal_garbage_collected\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2967\u001b[0m       \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2968\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2969\u001b[0;31m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2970\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2971\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   3359\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3360\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmissed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcall_context_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3361\u001b[0;31m           \u001b[0mgraph_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_graph_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3362\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprimary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3363\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[0;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m   3194\u001b[0m     \u001b[0marg_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbase_arg_names\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmissing_arg_names\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3195\u001b[0m     graph_function = ConcreteFunction(\n\u001b[0;32m-> 3196\u001b[0;31m         func_graph_module.func_graph_from_py_func(\n\u001b[0m\u001b[1;32m   3197\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3198\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_python_function\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[0;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m    988\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moriginal_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_decorator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munwrap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpython_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    989\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 990\u001b[0;31m       \u001b[0mfunc_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    991\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    992\u001b[0m       \u001b[0;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m    632\u001b[0m             \u001b[0mxla_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    633\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 634\u001b[0;31m           \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweak_wrapped_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__wrapped__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    635\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    636\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    975\u001b[0m           \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    976\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ag_error_metadata\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 977\u001b[0;31m               \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    978\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    979\u001b[0m               \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py:805 train_function  *\n        return step_function(self, iterator)\n    /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py:795 step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/tensorflow/python/distribute/distribute_lib.py:1259 run\n        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\n    /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/tensorflow/python/distribute/distribute_lib.py:2730 call_for_each_replica\n        return self._call_for_each_replica(fn, args, kwargs)\n    /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/tensorflow/python/distribute/distribute_lib.py:3417 _call_for_each_replica\n        return fn(*args, **kwargs)\n    /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py:788 run_step  **\n        outputs = model.train_step(data)\n    /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py:755 train_step\n        loss = self.compiled_loss(\n    /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/tensorflow/python/keras/engine/compile_utils.py:203 __call__\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/tensorflow/python/keras/losses.py:152 __call__\n        losses = call_fn(y_true, y_pred)\n    /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/tensorflow/python/keras/losses.py:256 call  **\n        return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/tensorflow/python/util/dispatch.py:201 wrapper\n        return target(*args, **kwargs)\n    /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/tensorflow/python/keras/losses.py:1608 binary_crossentropy\n        K.binary_crossentropy(y_true, y_pred, from_logits=from_logits), axis=-1)\n    /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/tensorflow/python/util/dispatch.py:201 wrapper\n        return target(*args, **kwargs)\n    /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/tensorflow/python/keras/backend.py:4979 binary_crossentropy\n        return nn.sigmoid_cross_entropy_with_logits(labels=target, logits=output)\n    /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/tensorflow/python/util/dispatch.py:201 wrapper\n        return target(*args, **kwargs)\n    /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/tensorflow/python/ops/nn_impl.py:173 sigmoid_cross_entropy_with_logits\n        raise ValueError(\"logits and labels must have the same shape (%s vs %s)\" %\n\n    ValueError: logits and labels must have the same shape ((None, 2) vs (None, 1))\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------------------------------------\n",
    "# SCRIPT CONFIGURATION\n",
    "# -------------------------------------------------------------\n",
    "TRAINING_EPOCHS = 150\n",
    "LEARNING_RATE = 0.0001\n",
    "SHUFFLE_ON_TRAINING = False\n",
    "CLASS_WEIGHTS = {0:1, 1:1}\n",
    "\n",
    "APPLY_FIT_VALIDATION = True\n",
    "K_FOLD_VALUE = 1\n",
    "TEST_ON_SPECIFIC_CODEBASE = None\n",
    "\n",
    "#APPLY_FIT_VALIDATION = False\n",
    "#K_FOLD_VALUE = 1\n",
    "#TEST_ON_SPECIFIC_CODEBASE = \"ldod-static\"\n",
    "\n",
    "EXPORT_MODEL = False\n",
    "\n",
    "CSV_FILE = \"../output/ldod-metrics-2021-05-05-16-26-35.csv\"\n",
    "CSV_ROWS = [\"Codebase\", \"Feature\", \"Cluster\", \"CLIP\", \"CRIP\", \"CROP\", \"CWOP\", \"CIP\", \"CDDIP\", \"COP\", \"CPIF\", \"CIOF\", \"SCCP\", \"FCCP\", \"Orchestrator\"]\n",
    "CSV_ROWS_TO_EXCLUDE = [\"Cluster\", \"CIP\", \"COP\", \"CIOF\", \"CPIF\", \"CROP\", \"CWOP\"]\n",
    "\n",
    "\n",
    "# -------------------------------------------------------------\n",
    "# EXECUTION\n",
    "# -------------------------------------------------------------\n",
    "dataset = read_dataset(CSV_FILE, CSV_ROWS, CSV_ROWS_TO_EXCLUDE)\n",
    "\n",
    "dataset_features = dataset.copy()\n",
    "dataset_labels = dataset_features.pop('Orchestrator')\n",
    "\n",
    "# generate a trace_features array to make the splitting of the batches easier\n",
    "trace_features = split_dataset_by_trace_features(dataset)\n",
    "random.shuffle(trace_features)\n",
    "\n",
    "# preprocessing\n",
    "inputs = create_input_objects(dataset_features)\n",
    "trace_preprocessing = create_preprocessing_logic(dataset, dataset_features, inputs)\n",
    "\n",
    "number_trace_features = len(trace_features)\n",
    "if K_FOLD_VALUE == 1:\n",
    "    training_features_size = int(number_trace_features - (number_trace_features*0.1))\n",
    "else:\n",
    "    training_features_size = int(number_trace_features - (number_trace_features/K_FOLD_VALUE))\n",
    "validation_features_size = int((number_trace_features - training_features_size) / 2) if APPLY_FIT_VALIDATION else 0\n",
    "testing_features_size = number_trace_features - training_features_size - validation_features_size\n",
    "\n",
    "print(f\"\\n\\nBatch size: {dataset_labels.size} | Number of trace features: {number_trace_features}\")\n",
    "print(f\"Training size: {training_features_size} | Validation size: {validation_features_size} | Testing size: {testing_features_size}\\n\\n\")\n",
    "\n",
    "\n",
    "histories = []\n",
    "labels = []\n",
    "predictions = []\n",
    "aucs = []\n",
    "losses = []\n",
    "recalls = []\n",
    "\n",
    "for iteration in range(K_FOLD_VALUE):\n",
    "  (training_batch_features, training_batch_labels), (testing_batch_features, testing_batch_labels), (validation_batch_features, validation_batch_labels) = get_kfold_iteration_batches(\n",
    "      iteration,\n",
    "      dataset_features,\n",
    "      dataset_labels,\n",
    "      trace_features,\n",
    "      training_features_size,\n",
    "      validation_features_size,\n",
    "      testing_features_size,\n",
    "  )\n",
    "  labels.append(testing_batch_labels)\n",
    "  \n",
    "  model = build_neural_network_model(\n",
    "      body = [\n",
    "          # layers.Dense(9, activation=\"relu\"),\n",
    "          layers.Dense(20, activation=\"relu\"),\n",
    "          layers.Dense(50, activation=\"relu\"),\n",
    "          layers.Dense(2, activation=\"softmax\")\n",
    "      ],\n",
    "      preprocessing_head = trace_preprocessing,\n",
    "      inputs = inputs,\n",
    "      # loss = \"sparse_categorical_crossentropy\",\n",
    "      loss=keras.losses.BinaryCrossentropy(),\n",
    "      optimizer = tf.optimizers.Adam(learning_rate=LEARNING_RATE), # Adam or SGD\n",
    "  )\n",
    "    \n",
    "  training_history = fit_neural_network(\n",
    "      model = model,\n",
    "      training_features = training_batch_features,\n",
    "      training_labels = training_batch_labels,\n",
    "      validation_features = validation_batch_features,\n",
    "      validation_labels = validation_batch_labels,\n",
    "      epochs = TRAINING_EPOCHS,\n",
    "      shuffle = SHUFFLE_ON_TRAINING,\n",
    "      weights = CLASS_WEIGHTS,\n",
    "  )\n",
    "  histories.append(training_history)\n",
    "\n",
    "  plot_training_results(training_history)\n",
    "\n",
    "  testing_predictions, test_loss, test_recall = test_model(\n",
    "      model = model,\n",
    "      testing_features = testing_batch_features,\n",
    "      testing_labels = testing_batch_labels,\n",
    "      verbose = False,\n",
    "  )\n",
    "  predictions.append(testing_predictions)\n",
    "    \n",
    "  losses.append(test_loss)\n",
    "  recalls.append(test_recall)\n",
    "\n",
    "  auc_value = plot_testing_results(testing_predictions, testing_batch_labels)\n",
    "  aucs.append(auc_value)\n",
    "\n",
    "  print(\"\\n\\n--------------------------------------------------------------------------------------\\n\\n\")\n",
    "\n",
    "sum_auc = 0.0\n",
    "for auc_value in aucs:\n",
    "    sum_auc += auc_value\n",
    "mean_auc = sum_auc / len(aucs)\n",
    "\n",
    "sum_losses = 0.0\n",
    "for loss in losses:\n",
    "    sum_losses += loss\n",
    "mean_loss = sum_losses / len(losses)\n",
    "\n",
    "sum_recalls = 0.0\n",
    "for recall in recalls:\n",
    "    sum_recalls += recall\n",
    "mean_recall = sum_recalls / len(recalls)\n",
    "\n",
    "print(f\"\\nMean Loss: {mean_loss}\\n\")\n",
    "print(f\"\\nMean Recall: {mean_recall}\\n\")\n",
    "print(f\"\\nMean AUC: {mean_auc}\\n\")\n",
    "\n",
    "\n",
    "if EXPORT_MODEL:\n",
    "  filename = f'trace_trained_model-{datetime.now().strftime(\"%d_%m_%Y_%H_%M_%S\")}'\n",
    "  model.save(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "trace_neural_network.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
